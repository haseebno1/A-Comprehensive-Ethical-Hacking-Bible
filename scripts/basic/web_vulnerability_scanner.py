#!/usr/bin/env python3
# web_vulnerability_scanner.py - A simple web vulnerability scanner for ethical hacking practice
# Compatible with both MacOS and Kali Linux

import argparse
import re
import requests
import sys
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# Define colors for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

# Disable SSL warnings
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

def get_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Simple Web Vulnerability Scanner for Ethical Hacking')
    parser.add_argument('-u', '--url', dest='url', help='Target URL to scan')
    parser.add_argument('-c', '--cookie', dest='cookie', help='Cookies to include with requests')
    parser.add_argument('--depth', dest='depth', type=int, default=1, help='Crawling depth (default: 1)')
    parser.add_argument('--timeout', dest='timeout', type=float, default=10.0, help='Request timeout in seconds (default: 10.0)')
    parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose output')
    return parser.parse_args()

def is_valid_url(url):
    """Check if the URL is valid"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False

def get_session(cookie=None):
    """Create and configure a requests session"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
    })
    
    if cookie:
        session.headers.update({'Cookie': cookie})
    
    return session

def get_links_from_url(session, url, timeout):
    """Extract all links from a URL"""
    links = []
    try:
        response = session.get(url, timeout=timeout, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                full_url = urljoin(url, href)
                # Only include links to the same domain
                if urlparse(url).netloc == urlparse(full_url).netloc:
                    links.append(full_url)
        return links
    except requests.exceptions.RequestException as e:
        print(f"{Colors.FAIL}[!] Error fetching URL {url}: {e}{Colors.ENDC}")
        return links

def get_forms_from_url(session, url, timeout):
    """Extract all forms from a URL"""
    forms = []
    try:
        response = session.get(url, timeout=timeout, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for form in soup.find_all('form'):
                form_details = {}
                form_details['action'] = urljoin(url, form.get('action', ''))
                form_details['method'] = form.get('method', 'get').lower()
                form_details['inputs'] = []
                
                for input_tag in form.find_all('input'):
                    input_type = input_tag.get('type', 'text')
                    input_name = input_tag.get('name')
                    input_value = input_tag.get('value', '')
                    
                    if input_name:
                        form_details['inputs'].append({
                            'type': input_type,
                            'name': input_name,
                            'value': input_value
                        })
                
                forms.append(form_details)
        return forms
    except requests.exceptions.RequestException as e:
        print(f"{Colors.FAIL}[!] Error fetching URL {url}: {e}{Colors.ENDC}")
        return forms

def test_xss(session, url, form, timeout, verbose):
    """Test for XSS vulnerabilities in a form"""
    xss_payloads = [
        '<script>alert("XSS")</script>',
        '<img src="x" onerror="alert(\'XSS\')">',
        '"><script>alert("XSS")</script>',
        '\'><script>alert("XSS")</script>',
        '<body onload="alert(\'XSS\')">',
        '<svg/onload=alert("XSS")>'
    ]
    
    original_form = form.copy()
    is_vulnerable = False
    
    for payload in xss_payloads:
        data = {}
        for input_data in form['inputs']:
            if input_data['type'] not in ['submit', 'hidden', 'button']:
                data[input_data['name']] = payload
            else:
                data[input_data['name']] = input_data['value']
        
        if verbose:
            print(f"{Colors.BLUE}[*] Trying XSS payload: {payload}{Colors.ENDC}")
        
        try:
            if form['method'] == 'post':
                response = session.post(form['action'], data=data, timeout=timeout, verify=False)
            else:
                response = session.get(form['action'], params=data, timeout=timeout, verify=False)
            
            # Check if the payload is reflected in the response
            if payload in response.text:
                print(f"{Colors.GREEN}[+] XSS vulnerability found in {form['action']}{Colors.ENDC}")
                print(f"{Colors.GREEN}[+] Form method: {form['method']}{Colors.ENDC}")
                print(f"{Colors.GREEN}[+] Payload: {payload}{Colors.ENDC}")
                is_vulnerable = True
                break
                
        except requests.exceptions.RequestException as e:
            if verbose:
                print(f"{Colors.FAIL}[!] Error testing XSS: {e}{Colors.ENDC}")
    
    return is_vulnerable

def test_sqli(session, url, form, timeout, verbose):
    """Test for SQL Injection vulnerabilities in a form"""
    sqli_payloads = [
        "' OR '1'='1",
        "' OR '1'='1' --",
        "' OR '1'='1' #",
        '" OR "1"="1',
        '" OR "1"="1" --',
        '" OR "1"="1" #',
        "') OR ('1'='1",
        "')) OR (('1'='1"
    ]
    
    error_patterns = [
        "SQL syntax",
        "mysql_fetch_array",
        "mysql_fetch_assoc",
        "mysql_num_rows",
        "mysql_fetch_row",
        "MySQL server",
        "syntax error",
        "ORA-",
        "Oracle error",
        "Microsoft SQL Server",
        "ODBC Driver",
        "SQLite3::"
    ]
    
    original_form = form.copy()
    is_vulnerable = False
    
    for payload in sqli_payloads:
        data = {}
        for input_data in form['inputs']:
            if input_data['type'] not in ['submit', 'hidden', 'button']:
                data[input_data['name']] = payload
            else:
                data[input_data['name']] = input_data['value']
        
        if verbose:
            print(f"{Colors.BLUE}[*] Trying SQLi payload: {payload}{Colors.ENDC}")
        
        try:
            if form['method'] == 'post':
                response = session.post(form['action'], data=data, timeout=timeout, verify=False)
            else:
                response = session.get(form['action'], params=data, timeout=timeout, verify=False)
            
            # Check for SQL error messages in the response
            for pattern in error_patterns:
                if pattern in response.text:
                    print(f"{Colors.GREEN}[+] SQL Injection vulnerability found in {form['action']}{Colors.ENDC}")
                    print(f"{Colors.GREEN}[+] Form method: {form['method']}{Colors.ENDC}")
                    print(f"{Colors.GREEN}[+] Payload: {payload}{Colors.ENDC}")
                    print(f"{Colors.GREEN}[+] Error pattern: {pattern}{Colors.ENDC}")
                    is_vulnerable = True
                    break
            
            if is_vulnerable:
                break
                
        except requests.exceptions.RequestException as e:
            if verbose:
                print(f"{Colors.FAIL}[!] Error testing SQLi: {e}{Colors.ENDC}")
    
    return is_vulnerable

def crawl(session, base_url, max_depth, timeout, verbose, visited=None, current_depth=0):
    """Crawl a website to a specified depth"""
    if visited is None:
        visited = set()
    
    if current_depth > max_depth:
        return visited
    
    if base_url in visited:
        return visited
    
    print(f"{Colors.BLUE}[*] Crawling: {base_url} (Depth: {current_depth}/{max_depth}){Colors.ENDC}")
    visited.add(base_url)
    
    links = get_links_from_url(session, base_url, timeout)
    if verbose:
        print(f"{Colors.BLUE}[*] Found {len(links)} links on {base_url}{Colors.ENDC}")
    
    for link in links:
        if link not in visited:
            # Add a small delay to avoid overwhelming the server
            time.sleep(0.5)
            crawl(session, link, max_depth, timeout, verbose, visited, current_depth + 1)
    
    return visited

def scan_url(session, url, timeout, verbose):
    """Scan a single URL for vulnerabilities"""
    print(f"{Colors.BLUE}[*] Scanning URL: {url}{Colors.ENDC}")
    
    # Get forms from the URL
    forms = get_forms_from_url(session, url, timeout)
    if verbose:
        print(f"{Colors.BLUE}[*] Found {len(forms)} forms on {url}{Colors.ENDC}")
    
    # Test each form for vulnerabilities
    for i, form in enumerate(forms):
        print(f"{Colors.BLUE}[*] Testing form {i+1}/{len(forms)} on {url}{Colors.ENDC}")
        
        # Test for XSS
        xss_vulnerable = test_xss(session, url, form, timeout, verbose)
        
        # Test for SQL Injection
        sqli_vulnerable = test_sqli(session, url, form, timeout, verbose)
        
        if not xss_vulnerable and not sqli_vulnerable and verbose:
            print(f"{Colors.WARNING}[-] No vulnerabilities found in form {i+1}/{len(forms)}{Colors.ENDC}")

def main():
    """Main function"""
    args = get_arguments()
    
    if not args.url:
        print(f"{Colors.FAIL}[!] Error: Target URL is required{Colors.ENDC}")
        sys.exit(1)
    
    if not is_valid_url(args.url):
        print(f"{Colors.FAIL}[!] Error: Invalid URL format{Colors.ENDC}")
        sys.exit(1)
    
    # Print banner
    print(f"{Colors.HEADER}{Colors.BOLD}Simple Web Vulnerability Scanner for Ethical Hacking{Colors.ENDC}")
    print(f"{Colors.HEADER}{'=' * 60}{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] Target URL: {args.url}{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] Crawling Depth: {args.depth}{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] Timeout: {args.timeout} seconds{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] Verbose: {args.verbose}{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] Start Time: {time.strftime('%Y-%m-%d %H:%M:%S')}{Colors.ENDC}")
    print(f"{Colors.HEADER}{'=' * 60}{Colors.ENDC}")
    
    # Create a session
    session = get_session(args.cookie)
    
    # Crawl the website
    print(f"{Colors.BLUE}[*] Starting crawl...{Colors.ENDC}")
    urls = crawl(session, args.url, args.depth, args.timeout, args.verbose)
    print(f"{Colors.GREEN}[+] Crawling complete. Found {len(urls)} unique URLs.{Colors.ENDC}")
    
    # Scan each URL for vulnerabilities
    print(f"{Colors.BLUE}[*] Starting vulnerability scan...{Colors.ENDC}")
    for url in urls:
        scan_url(session, url, args.timeout, args.verbose)
    
    print(f"{Colors.HEADER}{'=' * 60}{Colors.ENDC}")
    print(f"{Colors.BLUE}[*] End Time: {time.strftime('%Y-%m-%d %H:%M:%S')}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}Scan Complete{Colors.ENDC}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}[!] Scan interrupted by user{Colors.ENDC}")
        sys.exit(0)
